{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQecGb7rv475",
        "outputId": "1e6b4397-734a-4181-9d76-f1e48e250f6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDiJNE74wKk_",
        "outputId": "83ca8141-ddc8-429e-d307-cd605d128e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "id": "XsDVckdLv9E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5GaNxovv_FH",
        "outputId": "2aee386e-20c8-4015-fc16-b0cf9224d743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text(text, num_sentences=2):\n",
        "    # Tokenize text into sentences and words\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = word_tokenize(text.lower())\n",
        "\n",
        "    # Filter out stopwords and non-alphabetic words\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    word_frequencies = {}\n",
        "\n",
        "    for word in words:\n",
        "        if word.isalpha() and word not in stop_words:\n",
        "            word_frequencies[word] = word_frequencies.get(word, 0) + 1\n",
        "\n",
        "    # Score each sentence based on word frequency\n",
        "    sentence_scores = {}\n",
        "    for sentence in sentences:\n",
        "        sentence_words = word_tokenize(sentence.lower())\n",
        "        sentence_scores[sentence] = sum(word_frequencies.get(word, 0) for word in sentence_words)\n",
        "\n",
        "    # Sort sentences by score and select the top `num_sentences`\n",
        "    summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:num_sentences]\n",
        "    summary = \" \".join(summary_sentences)\n",
        "    return summary\n",
        "\n",
        "# Example text for summarization\n",
        "text = \"\"\"\n",
        "Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions.\n",
        "The term may also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving.\n",
        "\n",
        "The ideal characteristic of artificial intelligence is its ability to rationalize and take actions that have the best chance of achieving a specific goal.\n",
        "A subset of artificial intelligence is machine learning, which refers to the concept that computer programs can automatically learn from and adapt to new data without being assisted by humans.\n",
        "Deep learning techniques enable this automatic learning through the absorption of huge amounts of unstructured data such as text, images, or video.\n",
        "\"\"\"\n",
        "\n",
        "# Generate and print the summary\n",
        "summary = summarize_text(text, num_sentences=2)\n",
        "print(\"Original Text:\\n\", text)\n",
        "print(\"\\nSummary:\\n\", summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5w6eXVowC5D",
        "outputId": "30435d06-301f-4b26-f5a2-84c98e1f7ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            " \n",
            "Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions.\n",
            "The term may also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving.\n",
            "\n",
            "The ideal characteristic of artificial intelligence is its ability to rationalize and take actions that have the best chance of achieving a specific goal.\n",
            "A subset of artificial intelligence is machine learning, which refers to the concept that computer programs can automatically learn from and adapt to new data without being assisted by humans.\n",
            "Deep learning techniques enable this automatic learning through the absorption of huge amounts of unstructured data such as text, images, or video.\n",
            "\n",
            "\n",
            "Summary:\n",
            " A subset of artificial intelligence is machine learning, which refers to the concept that computer programs can automatically learn from and adapt to new data without being assisted by humans. \n",
            "Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary library\n",
        "!pip install nltk\n",
        "\n",
        "# Import necessary libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import os\n",
        "\n",
        "# Set the NLTK data path explicitly\n",
        "nltk.data.path.append('/root/nltk_data')\n",
        "\n",
        "# Ensure the directory exists\n",
        "if not os.path.exists('/root/nltk_data'):\n",
        "  os.makedirs('/root/nltk_data')\n",
        "\n",
        "# Download required resources (only needed once)\n",
        "nltk.download(\"stopwords\", download_dir='/root/nltk_data')\n",
        "nltk.download(\"punkt\", download_dir='/root/nltk_data')\n",
        "nltk.download(\"punkt_tab\", download_dir='/root/nltk_data')\n",
        "\n",
        "# Example text for summarization\n",
        "text = \"\"\"\n",
        "Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions.\n",
        "The term may also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving.\n",
        "\n",
        "The ideal characteristic of artificial intelligence is its ability to rationalize and take actions that have the best chance of achieving a specific goal.\n",
        "A subset of artificial intelligence is machine learning, which refers to the concept that computer programs can automatically learn from and adapt to new data without being assisted by humans.\n",
        "Deep learning techniques enable this automatic learning through the absorption of huge amounts of unstructured data such as text, images, or video.\n",
        "\"\"\"\n",
        "\n",
        "# Function to generate a frequency-based summary\n",
        "def summarize_text(text, num_sentences=2):\n",
        "    # Tokenize text into sentences and words\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = word_tokenize(text.lower())\n",
        "\n",
        "    # Filter out stopwords and non-alphabetic words\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    word_frequencies = {}\n",
        "\n",
        "    for word in words:\n",
        "        if word.isalpha() and word not in stop_words:\n",
        "            word_frequencies[word] = word_frequencies.get(word, 0) + 1\n",
        "\n",
        "    # Score each sentence based on word frequency\n",
        "    sentence_scores = {}\n",
        "    for sentence in sentences:\n",
        "        sentence_words = word_tokenize(sentence.lower())\n",
        "        sentence_scores[sentence] = sum(word_frequencies.get(word, 0) for word in sentence_words)\n",
        "\n",
        "    # Sort sentences by score and select the top `num_sentences`\n",
        "    summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:num_sentences]\n",
        "    summary = \" \".join(summary_sentences)\n",
        "    return summary\n",
        "\n",
        "# Generate and print the summary\n",
        "summary = summarize_text(text, num_sentences=2)\n",
        "print(\"Original Text:\\n\", text)\n",
        "print(\"\\nSummary:\\n\", summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2XfyYp4wUyu",
        "outputId": "9e8e564c-4327-487b-9454-b7b1a518157b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Original Text:\n",
            " \n",
            "Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. \n",
            "The term may also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving.\n",
            "\n",
            "The ideal characteristic of artificial intelligence is its ability to rationalize and take actions that have the best chance of achieving a specific goal. \n",
            "A subset of artificial intelligence is machine learning, which refers to the concept that computer programs can automatically learn from and adapt to new data without being assisted by humans. \n",
            "Deep learning techniques enable this automatic learning through the absorption of huge amounts of unstructured data such as text, images, or video.\n",
            "\n",
            "\n",
            "Summary:\n",
            " A subset of artificial intelligence is machine learning, which refers to the concept that computer programs can automatically learn from and adapt to new data without being assisted by humans. \n",
            "Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhM-ukQh7Hzc",
        "outputId": "0f3f9b3d-e769-45e0-88cc-0a6c19457cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install nltk gradio\n",
        "\n",
        "# Import necessary libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import gradio as gr\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "def summarize_text(text, num_sentences=2):\n",
        "    if not text.strip():\n",
        "        return \"No text provided. Please enter some input.\"\n",
        "\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = word_tokenize(text.lower())\n",
        "\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    word_frequencies = {}\n",
        "\n",
        "    for word in words:\n",
        "        if word.isalpha() and word not in stop_words:\n",
        "            word_frequencies[word] = word_frequencies.get(word, 0) + 1\n",
        "\n",
        "    sentence_scores = {}\n",
        "    for sentence in sentences:\n",
        "        sentence_words = word_tokenize(sentence.lower())\n",
        "        sentence_scores[sentence] = sum(word_frequencies.get(word, 0) for word in sentence_words)\n",
        "\n",
        "    if not sentence_scores:\n",
        "        return \"Could not compute summary. Please try with different text.\"\n",
        "\n",
        "    summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:num_sentences]\n",
        "    summary = \" \".join(summary_sentences)\n",
        "    return summary\n",
        "\n",
        "def summarize_interface(text, num_sentences):\n",
        "    try:\n",
        "        num_sent = int(num_sentences)\n",
        "        if num_sent <= 0:\n",
        "            return \"Please enter a number greater than 0.\"\n",
        "    except ValueError:\n",
        "        return \"Please enter a valid number for sentences.\"\n",
        "    return summarize_text(text, num_sent)\n",
        "\n",
        "# Create Gradio Interface\n",
        "demo = gr.Interface(\n",
        "    fn=summarize_interface,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=10, placeholder=\"Enter text to summarize...\", label=\"Input Text\"),\n",
        "        gr.Number(label=\"Summary Length (Number of Sentences)\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Summarized Text\"),\n",
        "    title=\"Text Summarizer\",\n",
        "    description=\"Enter text and specify how many sentences you want in the summary.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio app\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "As5EiC6d-QWA",
        "outputId": "add9dd61-648a-4731-fd1d-b2ad84e1b518",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.22.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.11)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.8.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.29.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.2)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c03b650ceee189bb02.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c03b650ceee189bb02.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries (only in Colab/Jupyter)\n",
        "# !pip install nltk gradio\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import gradio as gr\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "def summarize_text(text, num_sentences=2):\n",
        "    if not text.strip():\n",
        "        return \"No text provided. Please enter some input.\"\n",
        "\n",
        "    sentences = sent_tokenize(text)\n",
        "    if not sentences:\n",
        "        return \"The input text does not contain valid sentences.\"\n",
        "\n",
        "    words = word_tokenize(text.lower())\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    word_frequencies = {}\n",
        "\n",
        "    for word in words:\n",
        "        if word.isalpha() and word not in stop_words:\n",
        "            word_frequencies[word] = word_frequencies.get(word, 0) + 1\n",
        "\n",
        "    sentence_scores = {}\n",
        "    for sentence in sentences:\n",
        "        sentence_words = word_tokenize(sentence.lower())\n",
        "        score = sum(word_frequencies.get(word, 0) for word in sentence_words if word.isalpha())\n",
        "        if score > 0:\n",
        "            sentence_scores[sentence] = score / len(sentence_words)\n",
        "\n",
        "    if not sentence_scores:\n",
        "        return \"Could not compute summary. Please try with different text.\"\n",
        "\n",
        "    summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:num_sentences]\n",
        "    summary_sentences.sort(key=lambda s: sentences.index(s))  # Keep original order\n",
        "    return \" \".join(summary_sentences)\n",
        "\n",
        "def summarize_interface(text, num_sentences=2):\n",
        "    try:\n",
        "        num_sent = int(num_sentences)\n",
        "        if num_sent <= 0:\n",
        "            return \"Please enter a number greater than 0.\"\n",
        "    except Exception:\n",
        "        return \"Please enter a valid number.\"\n",
        "\n",
        "    return summarize_text(text, num_sent)\n",
        "\n",
        "# Create Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=summarize_interface,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=10, placeholder=\"Enter text to summarize...\", label=\"Input Text\"),\n",
        "        gr.Number(label=\"Summary Length (Number of Sentences)\", value=2)\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Summarized Output\"),\n",
        "    title=\"Text Summarizer\",\n",
        "    description=\"Enter text and select how many sentences should appear in the summary.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio app\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "o82n4lq6DMVg",
        "outputId": "65344141-4c67-49db-ba46-983676b592b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://983fdad10e9394cd3d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://983fdad10e9394cd3d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}